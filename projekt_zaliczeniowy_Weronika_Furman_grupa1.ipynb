{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projekt_zaliczeniowy_Weronika_Furman_grupa1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy -U \n",
        "!python -m spacy download pl_core_news_lg"
      ],
      "metadata": {
        "id": "0iJL6jn4KC-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZmER8eNL7I_F"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"pl_core_news_lg\")\n",
        "nlp_blank = spacy.blank(\"pl\")"
      ],
      "metadata": {
        "id": "-2ZooEf7Wk2A"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Część 1 - przetwarzanie i czyszczenie danych"
      ],
      "metadata": {
        "id": "6onk8L6tl5J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_twitter = pd.read_csv(\"/content/ani_jednej_wiecej.csv\")\n",
        "\n",
        "df_created_at = df_twitter[\"created_at\"]\n",
        "df_user_created_at = df_twitter[\"user_created_at\"]\n",
        "df_tweet_url = df_twitter[\"tweet_url\"]\n",
        "df_urls = df_twitter[\"urls\"]\n",
        "df_img_url = df_twitter[\"media\"]\n",
        "df_text = df_twitter[\"text\"]"
      ],
      "metadata": {
        "id": "t2c2e_GfWq8G"
      },
      "execution_count": 380,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1 zamiana nazw dni tygodnia\n",
        "\n",
        "days = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
        "days_pl = [\"Poniedziałek\", \"Wtorek\", \"Środa\", \"Czwartek\", \"Piątek\", \"Sobota\", \"Niedziela\"]\n",
        "\n",
        "df_twitter[\"created_at\"] = df_created_at.replace(days, days_pl, regex = True)\n",
        "df_created_at\n",
        "\n",
        "# df_twitter"
      ],
      "metadata": {
        "id": "1bvbR-f-XrII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 zamiana nazw miesięcy\n",
        "\n",
        "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
        "months_num = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
        "\n",
        "df_user_created_at = df_user_created_at.replace(months, months_num, regex = True)\n",
        "df_user_created_at\n",
        "\n",
        "# df_twitter"
      ],
      "metadata": {
        "id": "owMul3ZtqHlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 lista linków tweetów\n",
        "\n",
        "lista_linkow = [link for link in df_tweet_url]\n",
        "lista_linkow\n"
      ],
      "metadata": {
        "id": "hGhB_G7RuMcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 lista z linkami udostępnionymi w postach\n",
        "\n",
        "shared_url = [str(link) for link in df_urls] \n",
        "shared_url = [elem for elem in shared_url if elem != \"nan\"]\n",
        "\n",
        "print(shared_url)\n"
      ],
      "metadata": {
        "id": "5NG_taiHusXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 lista z linkami do obrazków\n",
        "\n",
        "img_url = [str(link) for link in df_img_url]\n",
        "img_url = [elem for elem in img_url if elem != \"nan\"] #usuwanie \"nan\" z listy\n",
        "\n",
        "print(img_url)"
      ],
      "metadata": {
        "id": "1oXaUKF-82sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 usuwanie stopwords\n",
        "\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "df_twitter['text_without_stopwords'] = df_twitter['text'].apply(lambda words: ' '.join([word for word in words.split() if word.lower() not in (stopwords)]))\n",
        "\n",
        "print(df_text, \"\\n\")\n",
        "print(df_twitter['text_without_stopwords'])\n"
      ],
      "metadata": {
        "id": "u-5zsCiN-mNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Część 2 - eksploracyjna analiza danych"
      ],
      "metadata": {
        "id": "ZeppKyyPUSFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fav = df_twitter[\"favorite_count\"]\n",
        "df_retweet = df_twitter[\"retweet_count\"]\n",
        "df_sensitive = df_twitter[\"possibly_sensitive\"]\n",
        "df_user_date = df_twitter[\"user_created_at\"]\n",
        "df_followers = df_twitter[\"user_followers_count\"]\n",
        "df_name = df_twitter[\"user_name\"]\n",
        "df_verified = df_twitter[\"user_verified\"]"
      ],
      "metadata": {
        "id": "P3SjLI1kUkBv"
      },
      "execution_count": 390,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1 top5 największa ilość polubień\n",
        "\n",
        "#sortuję listę od największej wartości do najmniejszj\n",
        "top5_fav = df_fav.sort_values(ascending=False).head()\n",
        "\n",
        "top = 0\n",
        "for index, number in top5_fav.items():\n",
        "  top +=1\n",
        "  print(f\"\"\"\n",
        "  Top {top}\n",
        "  Liczba polubień {number}: \n",
        "  {df_text[index]}\"\"\")"
      ],
      "metadata": {
        "id": "IE9wwdNZUV3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 top5 największa ilość retweetow\n",
        "\n",
        "#sotuję listę i usuwam duplikaty (top 5 było powieleniem tego samego tweeta)\n",
        "top5_retweet = df_retweet.sort_values(ascending=False).drop_duplicates().head()\n",
        "\n",
        "top = 0\n",
        "for index, number in top5_retweet.items():\n",
        "  top +=1\n",
        "  print(f\"\"\"\n",
        "  Top {top}\n",
        "  Liczba retweetów: {number}\n",
        "  {df_text[index]}\"\"\")\n",
        "  "
      ],
      "metadata": {
        "id": "V_lqlPIQUdd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Wyświetlanie tweetów nieuznanych jako wrażliwych\n",
        "\n",
        "not_sensitive = df_sensitive.where(df_sensitive == False).dropna()\n",
        "\n",
        "for index, value in not_sensitive.items():\n",
        "  tweet = df_text[index]\n",
        "  print(index, \"\\n\", tweet, \"\\n\")\n",
        "  "
      ],
      "metadata": {
        "id": "AELOTkHgejyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 użytkownik, który najwcześniej założył konto\n",
        "\n",
        "#stworzyłam nową kolumnę, w której przechowywany jest rok założenia konta\n",
        "#oprócz roku, wyszukiwana była też fraza '0000' występująca przed rokiem, dlatego istotny jest tylko jeden element z wyszukanej listy\n",
        "df_twitter[\"birth_year\"] = [re.findall(\"\\d{4}\", elem)[1] for elem in df_user_date]\n",
        "\n",
        "#okazało się, że tylko jeden użytkownik założył konto w 2006 roku i nie było potrzeby sprawdzania różnicy w miesiącach i dniach\n",
        "year_created = df_twitter[\"birth_year\"].sort_values().head(1)\n",
        "\n",
        "for index, year in year_created.items():\n",
        "    print(f\"\"\"\n",
        "    {df_name[index]}\n",
        "    rok założenia konta: {year}\n",
        "    {df_text[index]}\"\"\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mghg0jPkjSGY",
        "outputId": "84bc0c03-9698-4d52-9551-71271cdb3925"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Hex 🇵🇱 #PolskiŁad #MuremZaPolskimMundurem\n",
            "    rok założenia konta: 2006\n",
            "    #AniJednejWięcej. Jak politycy manipulują przepisami i treścią wyroku TK - https://t.co/MuUX62CedH https://t.co/yn3o7MuNsm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 tweety użytkownika z największą ilością followersów\n",
        "\n",
        "user_max = df_followers.max()\n",
        "\n",
        "for index, followers in df_followers.items():\n",
        "  if followers == user_max:\n",
        "    print(f\"\"\"\n",
        "    {df_name[index]} ({user_max} followersów)\n",
        "    {df_text[index]}\n",
        "    \"\"\")\n"
      ],
      "metadata": {
        "id": "0-epu3sprV1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 wwyświetlanie zweryfikowanych użytkowników\n",
        "\n",
        "verified_users_index = df_verified.where(df_verified == True).dropna()\n",
        "# print(verified_users_index)\n",
        "\n",
        "verified_users = [df_name[index] for index, verification in verified_users_index.items()]\n",
        "# print(verified_users)\n",
        "\n",
        "#tworzę listę bez duplikatów\n",
        "users_no_duplicates = []\n",
        "\n",
        "for name in verified_users:\n",
        "  if name not in users_no_duplicates:\n",
        "    users_no_duplicates.append(name)\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "gwyejTVLmSWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 najpopularniejszy dzień tygodnia publikowania\n",
        "\n",
        "week = {\n",
        "    \"poniedziałek\": df_created_at.str.count(r'Poniedziałek').sum(),\n",
        "    \"wtorek\": df_created_at.str.count(r'(Wtorek)').sum(),\n",
        "    \"środa\": df_created_at.str.count(r'(Środa)').sum(),\n",
        "    \"czwartek\": df_created_at.str.count(r\"(Czwartek)\").sum(),\n",
        "    \"piątek\": df_created_at.str.count(r'(Piątek)').sum(),\n",
        "    \"sobota\": df_created_at.str.count(r'(Sobota)').sum(),\n",
        "    \"niedziela\": df_created_at.str.count(r'(Niedziela)').sum()\n",
        "    }\n",
        "\n",
        "# print(week)\n",
        "\n",
        "all_values = week.values()\n",
        "max_value = max(all_values)\n",
        "\n",
        "for day, value in week.items():\n",
        "  if value == max_value:\n",
        "    print(f\"\"\"\n",
        "    Dzień tygodnia, w którym zostało opublikowanych najwięcej tweetów: {day}\n",
        "    liczba tweetów: {value}\"\"\")\n"
      ],
      "metadata": {
        "id": "-zoWCQ1mr-7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Część 3 - przetwarzanie języka naturalnego"
      ],
      "metadata": {
        "id": "m0GuqNwvzrrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_1 = [nlp(text) for text in df_text]"
      ],
      "metadata": {
        "id": "LFoOhCb_MIlW"
      },
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 rozpoznawanie osób\n",
        "\n",
        "people = []\n",
        "\n",
        "#lista excluded_people powstała po analizie wykrytych słów\n",
        "excluded_people = ['jprdl https://t.co/RZ6epYW0a1', 'https://t.co/BmEKSFfV3g', 'https://t.co/RZilLymkzp', 'https://t.co/PuBPs7XP6T', '✍️', 'https://t.co/lWOqq1gcEY', 'https://t.co/LZecWajv4b' ]\n",
        "\n",
        "for tweet in doc_1:\n",
        "  osoba = \"\"\n",
        "  for ent in tweet.ents:\n",
        "    if ent.label_ == \"persName\" and ent.text not in excluded_people:\n",
        "      osoba = osoba + ent.text + \" \"\n",
        "  people.append(osoba)\n",
        "\n",
        "assert len(people) == df_twitter.shape[0]\n",
        "df_twitter[\"persons\"] = people\n",
        "\n",
        "# people_values = df_twitter[\"persons\"].value_counts()\n",
        "# print(people_values[0:50])\n",
        "\n",
        "print(df_twitter['persons'])\n"
      ],
      "metadata": {
        "id": "f6u5gzcdkD7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 rozpoznawanie miejsc\n",
        "\n",
        "places = []\n",
        "\n",
        "#zmienna excluded_people powstała po analizie wykrytych słów\n",
        "excluded_places = \"ŻE\" #zwroty, które nie są miejscami a były identyfikowane\n",
        "\n",
        "for tweet in doc_1:\n",
        "  miejsce = \"\"\n",
        "  for ent in tweet.ents:\n",
        "    if ent.label_ == \"placeName\" and ent.text != excluded_places:\n",
        "      miejsce = miejsce + ent.text + \" \" \n",
        "  places.append(miejsce)\n",
        "\n",
        "assert len(places) == df_twitter.shape[0]\n",
        "df_twitter[\"places\"] = places\n",
        "\n",
        "# places_values = df_twitter[\"places\"].value_counts()\n",
        "# print(places_values[0:50])\n",
        "\n",
        "print(df_twitter['places'][0:50])\n"
      ],
      "metadata": {
        "id": "sqHVtB7D1IGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 rozpoznawanie organizacji\n",
        "\n",
        "org = []\n",
        "\n",
        "for tweet in doc_1:\n",
        "  organization = \"\"\n",
        "  for ent in tweet.ents:\n",
        "    if ent.label_ == \"orgName\":\n",
        "      organization = organization + ent.text + \" \" \n",
        "  org.append(organization)\n",
        "\n",
        "assert len(org) == df_twitter.shape[0]\n",
        "df_twitter[\"organizations\"] = org\n",
        "\n",
        "# org_values = df_twitter[\"organizations\"].value_counts()\n",
        "# print(org_values[0:50])\n",
        "\n",
        "print(df_twitter[\"organizations\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "zwpRL19a1JiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 rozpoznawanie hashtagów\n",
        "\n",
        "ruler = nlp_blank.add_pipe(\"entity_ruler\")\n",
        "\n",
        "pattern = \"\\S+\"\n",
        "patterns = [{\"label\": \"hashtag\", \"pattern\": [{\"ORTH\": \"#\"}, {\"TEXT\": {\"REGEX\": pattern}}]}]\n",
        "\n",
        "ruler.add_patterns(patterns)"
      ],
      "metadata": {
        "id": "7Y2fVdDc39w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_2 = [nlp_blank(text) for text in df_twitter[\"text\"]]"
      ],
      "metadata": {
        "id": "co2cCRqMUz8W"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtags = []\n",
        "\n",
        "for tweet in doc_2:\n",
        "  hashtag = \"\"\n",
        "  for ent in tweet.ents:\n",
        "    if ent.label_ == \"hashtag\":\n",
        "      hashtag = hashtag + ent.text + \" \" \n",
        "  hashtags.append(hashtag)\n",
        "\n",
        "assert len(hashtags) == df_twitter.shape[0]\n",
        "df_twitter[\"hashtags\"] = hashtags\n",
        "\n",
        "# hash_values = df_twitter[\"hashtags\"].value_counts()\n",
        "# print(hash_values)\n",
        "\n",
        "print(df_twitter['hashtags'])"
      ],
      "metadata": {
        "id": "VvfWZZMsSPbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Część 4 - rozwiązywanie problemów/umiejętność interpretowania dokumentacji"
      ],
      "metadata": {
        "id": "xBW6idvLUYmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#wykres liczba tweetów per dni tygodnia\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"Liczba tweetów opublikowanych według dni tygodnia\", fontsize = 20)\n",
        "plt.xlabel('Dni tygodnia', fontsize = 14)\n",
        "plt.ylabel('Liczba tweetów', fontsize = 14)\n",
        "plot = plt.bar(*zip(*week.items()), color = 'g', width = 0.5)"
      ],
      "metadata": {
        "id": "lQnb3zNLtkAj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}